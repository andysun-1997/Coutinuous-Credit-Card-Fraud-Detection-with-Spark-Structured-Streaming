{"cells":[{"cell_type":"markdown","source":["# Writing Continuous Applications with Structured Streaming Python APIs in Apache Spark\n\nTutorial for Spark Streaming\n\n\nAt first glance, building a distributed streaming engine might seem as simple as launching a set of servers and pushing data between them. Unfortunately, distributed stream processing runs into multiple complications that don’t affect simpler computations like batch jobs. Fortunately, PySpark 2.4 and Databricks makes this simple!\n\n**Orignal Author**: Michael John\n\nModified and Ported to PySpark by Jules S. Damji and Mike for the Tutorial"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<div style=\"line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/12/PySparkStructuredStreaming_FINAL.jpg\" alt=\"Structrured Streaming\" width=\"40%\" style=>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Setup Data"],"metadata":{}},{"cell_type":"code","source":["%run \"./setup/setup_data\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### PySpark Documentation: [Click here](https://spark.apache.org/docs/latest/api/python/index.html)\n * [DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame)\n * [Spark SQL](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)\n * [Spark SQL Functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"],"metadata":{}},{"cell_type":"markdown","source":["# Processing Streaming Sensor Data\n\nStructured Streaming is a powerful capability for building end-to-end continuous applications. At a high-level, it offers the following features:\n\n1. __Output tables are always consistent__ with all the records in a prefix (partition) of the data, we will process and count in order.\n1. __Fault tolerance__ is handled holistically by Structured Streaming, including in interactions with output sinks.\n1. Ability to handle __late and out-of-order event-time data__. \n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/continuous-apps-1024x366.png\" alt=\"\" width=\"40%\"/>\n\n<sub>reference [Structured Streaming Blog](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)</sub>"],"metadata":{}},{"cell_type":"markdown","source":["## 1-Sources\n\nConsider the input data stream as the “Input Table”. Every data item that is arriving on the stream is like a new row being appended to the Input Table.\n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/cloudtrail-unbounded-tables.png\" alt=\"\" width=\"40%\"/>"],"metadata":{}},{"cell_type":"markdown","source":["## 2-Continuous Processing & Queries\n\nThe developer then defines a query on this source, or input table, _as if it were a static table_ to compute a final result table that will be written to an output sink. Spark automatically converts this batch-like query to a streaming execution plan. This is called incrementalization: Spark figures out what state needs to be maintained to update the result each time a record arrives. Finally, developers specify triggers to control when to update the results. Each time a trigger fires, Spark checks for new data (new row in the input table), and incrementally updates the result.\n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/cloudtrail-structured-streaming-model.png\" alt=\"\" width=\"40%\"/>"],"metadata":{}},{"cell_type":"markdown","source":["## 3-Sinks\n\nThe last part of the model is output modes. Each time the result table is updated, the developer wants to write the changes to an external system, such as S3, HDFS, or a database. We usually want to write output incrementally. For this purpose, Structured Streaming provides three output modes:\n\n* __Append__: Only the new rows appended to the result table since the last trigger will be written to the external storage. \n* __Complete__: The entire updated result table will be written to external storage, e.g. for aggregates.\n* __Update__: Only the rows that were updated in the result table since the last trigger will be changed in the external storage. \n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/stream-example1-phone-updated.png\" width=\"40%\"/>"],"metadata":{}},{"cell_type":"markdown","source":["## A Continuous Application Example using PySpark Structured Streaming APIs"],"metadata":{}},{"cell_type":"markdown","source":["Setup some file paths our S3 bucket for output, checkpoint, and bad records"],"metadata":{}},{"cell_type":"code","source":["output_path = \"/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/\"\ncheckpoint_path = \"/tmp/pydata/Streaming/continuous_streaming/out/iot-stream-checkpoint\"\n#\n#create checkpoint path\n#\ndbutils.fs.rm(checkpoint_path,True) #overwrite checkpoint\ndbutils.fs.mkdirs(checkpoint_path)\n#\n#\nbad_records_path = \"/tmp/pydata/Streaming/continuous_streaming/badRecordsPath/streaming-sensor/\"\ndbutils.fs.rm(bad_records_path, True) #empty dir\ndbutils.fs.mkdirs(bad_records_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: True</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### What does the data from the sensors look like?"],"metadata":{}},{"cell_type":"code","source":["sensor_path = \"/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/\"\nsensor_file_name = sensor_path + \"streaming-sensor_file-1.json\"\ndbutils.fs.head(sensor_file_name, 233)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Truncated to first 233 bytes]\nOut[3]: &#39;{&#34;timestamp&#34;:&#34;2016-08-03 01:32:20.454&#34;,&#34;deviceId&#34;:44,&#34;deviceType&#34;:&#34;SensorTypeD&#34;,&#34;signalStrength&#34;:0.6627327869584749}\\n{&#34;timestamp&#34;:&#34;2016-08-03 01:32:30.579&#34;,&#34;deviceId&#34;:63,&#34;deviceType&#34;:&#34;SensorTypeB&#34;,&#34;signalStrength&#34;:0.6419303214810813}&#39;</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["### Define schemas for incoming stream and outgoing stream\n\nGood best practice to define a schema and not have Spark infer it, for performance reasons. \nWithout a schema, Spark will launch couple of jobs: one to read header, and another to read\na good chuck of the partition to validate the schema, ensuring it matches the data.\n\nNow there are options that you can specifiy so that it fails fast, is tolerant, or subsitute missing\n values or mismatch dataytpes with NaN or null."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n#original input schema\njsonSchema = (\n  StructType()\n  .add(\"timestamp\", TimestampType()) #event time at the source\n  .add(\"deviceId\", LongType())\n  .add(\"deviceType\", StringType())\n  .add(\"signalStrength\", DoubleType())\n)\n# modified schema with added columns since we are \n# doing some ETL (transforming and adding extra columns)\n# this transformed data will be stored into parquet files\n# from which an SQL table can be created for consumption or\n# report generation\nparquetSchema = (\n  StructType()\n  .add(\"timestamp\", TimestampType()) #event time at the source\n  .add(\"deviceId\", LongType())\n  .add(\"deviceType\", StringType())\n  .add(\"signalStrength\", DoubleType())\n  .add(\"INPUT_FILE_NAME\", StringType()) #file name from which this data item was read\n  .add(\"PROCESSED_TIME\", TimestampType())) #time at the executor while processing"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### Read Stream from Object Store Source\n\nIn this case we're simulating a Kafka live stream by reading in a file at a time. But this could as well be Apache Kafka topics\n\n__Notice: Intentionally slowing this down for tutorial.__"],"metadata":{}},{"cell_type":"code","source":["inputDF = ( spark \n          .readStream \n          .schema(jsonSchema) \n          .option(\"maxFilesPerTrigger\", 1)  #slow it down for tutorial\n          .option(\"badRecordsPath\", bad_records_path) #any bad records will go here\n          .json(sensor_path) #the source\n          .withColumn(\"INPUT_FILE_NAME\", input_file_name()) #maintain file path\n          .withColumn(\"PROCESSED_TIME\", current_timestamp()) #add a processing timestamp at the time of processing\n          .withWatermark(\"PROCESSED_TIME\", \"1 minute\") #optional: window for out of order data\n         )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["### Write Stream to Parquet File Sink"],"metadata":{}},{"cell_type":"code","source":["query = (inputDF\n         .writeStream\n         .format(\"parquet\") #our sink to save it for posterity or batch queries if needed\n         .option(\"path\", output_path)\n         .option(\"checkpointLocation\", checkpoint_path) # add checkpointing for resiliency\n         .outputMode(\"append\")\n         .queryName(\"devices\") #optionally a query name over write to issue queries against\n         .trigger(processingTime='5 seconds')\n         .start() \n        )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["%fs ls /tmp/pydata/Streaming/continuous_streaming/out/iot-stream/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/_spark_metadata/</td><td>_spark_metadata/</td><td>0</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-00d7d4a1-c46c-4141-8b68-2fbe016c3bd0-c000.snappy.parquet</td><td>part-00000-00d7d4a1-c46c-4141-8b68-2fbe016c3bd0-c000.snappy.parquet</td><td>4718</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-010d3489-2637-4c38-a737-7feebaf9e10e-c000.snappy.parquet</td><td>part-00000-010d3489-2637-4c38-a737-7feebaf9e10e-c000.snappy.parquet</td><td>4754</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-02b5aa63-cbb1-4a02-8986-c570cb4b8bb6-c000.snappy.parquet</td><td>part-00000-02b5aa63-cbb1-4a02-8986-c570cb4b8bb6-c000.snappy.parquet</td><td>4756</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-0300f81e-77c0-4f06-9b01-45bbdf0ba583-c000.snappy.parquet</td><td>part-00000-0300f81e-77c0-4f06-9b01-45bbdf0ba583-c000.snappy.parquet</td><td>4713</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-03e6acbd-4ee1-4a4b-bab3-28e43d07edec-c000.snappy.parquet</td><td>part-00000-03e6acbd-4ee1-4a4b-bab3-28e43d07edec-c000.snappy.parquet</td><td>4755</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-0832d227-b6c4-47e6-bead-c8345e5df620-c000.snappy.parquet</td><td>part-00000-0832d227-b6c4-47e6-bead-c8345e5df620-c000.snappy.parquet</td><td>4730</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-09f2e79e-9bf8-44e8-a77d-5ad96defabe9-c000.snappy.parquet</td><td>part-00000-09f2e79e-9bf8-44e8-a77d-5ad96defabe9-c000.snappy.parquet</td><td>4750</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-0b49bdee-dea5-4597-bca5-f518cb59ddb0-c000.snappy.parquet</td><td>part-00000-0b49bdee-dea5-4597-bca5-f518cb59ddb0-c000.snappy.parquet</td><td>4713</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-0ba6f60b-237b-4273-af46-c1a7155c49a4-c000.snappy.parquet</td><td>part-00000-0ba6f60b-237b-4273-af46-c1a7155c49a4-c000.snappy.parquet</td><td>4762</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-0bfffbd7-f59e-4d7d-bdb1-eb1df90079a0-c000.snappy.parquet</td><td>part-00000-0bfffbd7-f59e-4d7d-bdb1-eb1df90079a0-c000.snappy.parquet</td><td>4756</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-0e05a461-2241-4efe-8b4b-6881cb8e5eb2-c000.snappy.parquet</td><td>part-00000-0e05a461-2241-4efe-8b4b-6881cb8e5eb2-c000.snappy.parquet</td><td>4726</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-1050e5cc-2ec8-48af-a02d-8b86f5d8a9f0-c000.snappy.parquet</td><td>part-00000-1050e5cc-2ec8-48af-a02d-8b86f5d8a9f0-c000.snappy.parquet</td><td>4729</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-146e00e1-9ee0-461d-9168-3a330bed612b-c000.snappy.parquet</td><td>part-00000-146e00e1-9ee0-461d-9168-3a330bed612b-c000.snappy.parquet</td><td>4769</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-1884b39b-2d96-4d37-a524-8c06cde1b24a-c000.snappy.parquet</td><td>part-00000-1884b39b-2d96-4d37-a524-8c06cde1b24a-c000.snappy.parquet</td><td>4713</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-1c2cc95d-786a-430b-8665-5a146af46de2-c000.snappy.parquet</td><td>part-00000-1c2cc95d-786a-430b-8665-5a146af46de2-c000.snappy.parquet</td><td>4767</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-205156d2-cd96-4ccf-8ce1-4be05a0c124c-c000.snappy.parquet</td><td>part-00000-205156d2-cd96-4ccf-8ce1-4be05a0c124c-c000.snappy.parquet</td><td>4733</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-2086a68d-7479-4444-a268-9ee0645c29c6-c000.snappy.parquet</td><td>part-00000-2086a68d-7479-4444-a268-9ee0645c29c6-c000.snappy.parquet</td><td>4717</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-2270bd6a-1ac9-4ffe-bbe7-eccad7960e36-c000.snappy.parquet</td><td>part-00000-2270bd6a-1ac9-4ffe-bbe7-eccad7960e36-c000.snappy.parquet</td><td>4734</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-2a7a8b54-5c7a-4e5a-92c1-402ddd76ea4b-c000.snappy.parquet</td><td>part-00000-2a7a8b54-5c7a-4e5a-92c1-402ddd76ea4b-c000.snappy.parquet</td><td>4767</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-2cb5fa0c-2108-4243-9ef7-bbf63d9d42d6-c000.snappy.parquet</td><td>part-00000-2cb5fa0c-2108-4243-9ef7-bbf63d9d42d6-c000.snappy.parquet</td><td>4758</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-2e28a664-f6ad-4634-92bb-afcf8c043210-c000.snappy.parquet</td><td>part-00000-2e28a664-f6ad-4634-92bb-afcf8c043210-c000.snappy.parquet</td><td>4720</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-2f79a519-1a76-49e5-9592-67d889c0d24f-c000.snappy.parquet</td><td>part-00000-2f79a519-1a76-49e5-9592-67d889c0d24f-c000.snappy.parquet</td><td>4720</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-34228762-58b6-4210-b1c0-04d55e80a3c5-c000.snappy.parquet</td><td>part-00000-34228762-58b6-4210-b1c0-04d55e80a3c5-c000.snappy.parquet</td><td>4705</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-34597f68-8412-4cfe-a32d-89b5284c4e7f-c000.snappy.parquet</td><td>part-00000-34597f68-8412-4cfe-a32d-89b5284c4e7f-c000.snappy.parquet</td><td>4714</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-3c0ab910-8dcc-44be-b809-6edc317d555b-c000.snappy.parquet</td><td>part-00000-3c0ab910-8dcc-44be-b809-6edc317d555b-c000.snappy.parquet</td><td>4726</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-3ca79a11-58d9-48a9-b69d-6e7cc157bbf0-c000.snappy.parquet</td><td>part-00000-3ca79a11-58d9-48a9-b69d-6e7cc157bbf0-c000.snappy.parquet</td><td>4735</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-3de01f6a-35f1-4e4f-a116-1b5fa58acacd-c000.snappy.parquet</td><td>part-00000-3de01f6a-35f1-4e4f-a116-1b5fa58acacd-c000.snappy.parquet</td><td>4696</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-3f61f280-3b86-44a5-86d8-e1f9a2ca21cf-c000.snappy.parquet</td><td>part-00000-3f61f280-3b86-44a5-86d8-e1f9a2ca21cf-c000.snappy.parquet</td><td>4737</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-421512bb-2faf-4814-b40a-6467aa55e269-c000.snappy.parquet</td><td>part-00000-421512bb-2faf-4814-b40a-6467aa55e269-c000.snappy.parquet</td><td>4722</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-4758a37a-c950-474a-b14c-241fa5948b22-c000.snappy.parquet</td><td>part-00000-4758a37a-c950-474a-b14c-241fa5948b22-c000.snappy.parquet</td><td>4711</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-4991846f-99df-48ed-97db-8516a9928de3-c000.snappy.parquet</td><td>part-00000-4991846f-99df-48ed-97db-8516a9928de3-c000.snappy.parquet</td><td>4725</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-49c69bd7-bd60-416f-9f8f-6b9eea869372-c000.snappy.parquet</td><td>part-00000-49c69bd7-bd60-416f-9f8f-6b9eea869372-c000.snappy.parquet</td><td>4717</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-4cdc968d-4f56-4997-910d-87312420e81f-c000.snappy.parquet</td><td>part-00000-4cdc968d-4f56-4997-910d-87312420e81f-c000.snappy.parquet</td><td>4718</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-4fc7e85c-ba88-4b00-9a48-356dcfcc0b9c-c000.snappy.parquet</td><td>part-00000-4fc7e85c-ba88-4b00-9a48-356dcfcc0b9c-c000.snappy.parquet</td><td>4723</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-51297e5a-f420-4a4b-a4d7-eeff6f8bd2c5-c000.snappy.parquet</td><td>part-00000-51297e5a-f420-4a4b-a4d7-eeff6f8bd2c5-c000.snappy.parquet</td><td>4726</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-570e2dd0-3dd0-40cc-af80-9f532c3c1d7a-c000.snappy.parquet</td><td>part-00000-570e2dd0-3dd0-40cc-af80-9f532c3c1d7a-c000.snappy.parquet</td><td>4727</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-5f269f37-2c0d-4969-b1cc-ed6317d5c375-c000.snappy.parquet</td><td>part-00000-5f269f37-2c0d-4969-b1cc-ed6317d5c375-c000.snappy.parquet</td><td>4706</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-60e02542-b7da-4244-892a-b6560f4c7ee8-c000.snappy.parquet</td><td>part-00000-60e02542-b7da-4244-892a-b6560f4c7ee8-c000.snappy.parquet</td><td>4724</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-61f104d3-3d1e-4606-a3c9-fc5c69b9f87e-c000.snappy.parquet</td><td>part-00000-61f104d3-3d1e-4606-a3c9-fc5c69b9f87e-c000.snappy.parquet</td><td>4711</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-61f50180-6591-4a65-9ed8-a0d9508aa1d0-c000.snappy.parquet</td><td>part-00000-61f50180-6591-4a65-9ed8-a0d9508aa1d0-c000.snappy.parquet</td><td>4710</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-6419cebe-0529-4b2a-8069-38ac585331cd-c000.snappy.parquet</td><td>part-00000-6419cebe-0529-4b2a-8069-38ac585331cd-c000.snappy.parquet</td><td>4743</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-66f79bcb-ba45-4bef-9bfd-4aaa60350949-c000.snappy.parquet</td><td>part-00000-66f79bcb-ba45-4bef-9bfd-4aaa60350949-c000.snappy.parquet</td><td>4768</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-677a6548-f431-4861-991e-016f53dbe5b0-c000.snappy.parquet</td><td>part-00000-677a6548-f431-4861-991e-016f53dbe5b0-c000.snappy.parquet</td><td>4697</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-67cd864b-7fda-4acb-b625-af53e6ac62ae-c000.snappy.parquet</td><td>part-00000-67cd864b-7fda-4acb-b625-af53e6ac62ae-c000.snappy.parquet</td><td>4727</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-6c0ce810-8dd8-4564-ba8a-09bfd1faa0dc-c000.snappy.parquet</td><td>part-00000-6c0ce810-8dd8-4564-ba8a-09bfd1faa0dc-c000.snappy.parquet</td><td>4743</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-6c11b8c4-e045-466b-a0ab-3d80af28f415-c000.snappy.parquet</td><td>part-00000-6c11b8c4-e045-466b-a0ab-3d80af28f415-c000.snappy.parquet</td><td>4721</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-6fecb590-a3a7-456b-bf0d-26e0f3c8d71f-c000.snappy.parquet</td><td>part-00000-6fecb590-a3a7-456b-bf0d-26e0f3c8d71f-c000.snappy.parquet</td><td>4746</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-70822560-2500-4d80-b224-f80e92de9a9e-c000.snappy.parquet</td><td>part-00000-70822560-2500-4d80-b224-f80e92de9a9e-c000.snappy.parquet</td><td>4755</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-71732f6c-89e9-481c-b4e8-b2fb250f9c13-c000.snappy.parquet</td><td>part-00000-71732f6c-89e9-481c-b4e8-b2fb250f9c13-c000.snappy.parquet</td><td>4767</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-762b3e13-4d9e-4a73-95d4-ac197ef3f0e9-c000.snappy.parquet</td><td>part-00000-762b3e13-4d9e-4a73-95d4-ac197ef3f0e9-c000.snappy.parquet</td><td>4727</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-77b30722-7ea1-44f1-a178-3438df35e394-c000.snappy.parquet</td><td>part-00000-77b30722-7ea1-44f1-a178-3438df35e394-c000.snappy.parquet</td><td>4722</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-78127212-d2b5-4a02-9a0b-fb0738c0f1a0-c000.snappy.parquet</td><td>part-00000-78127212-d2b5-4a02-9a0b-fb0738c0f1a0-c000.snappy.parquet</td><td>4754</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-7aeb2f27-22d8-43dd-bad5-c72d2a87d0fd-c000.snappy.parquet</td><td>part-00000-7aeb2f27-22d8-43dd-bad5-c72d2a87d0fd-c000.snappy.parquet</td><td>4724</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-7b8aa1df-75f5-4264-b952-2f5a83a9f7e8-c000.snappy.parquet</td><td>part-00000-7b8aa1df-75f5-4264-b952-2f5a83a9f7e8-c000.snappy.parquet</td><td>4726</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-83e5eaa0-3131-41d3-9020-b92b6d595e53-c000.snappy.parquet</td><td>part-00000-83e5eaa0-3131-41d3-9020-b92b6d595e53-c000.snappy.parquet</td><td>4756</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-8a4c4727-7879-44d6-9d21-c0cc9c67e330-c000.snappy.parquet</td><td>part-00000-8a4c4727-7879-44d6-9d21-c0cc9c67e330-c000.snappy.parquet</td><td>4705</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-8aac1c81-c2bd-41e8-b4cb-d6245ce40d4a-c000.snappy.parquet</td><td>part-00000-8aac1c81-c2bd-41e8-b4cb-d6245ce40d4a-c000.snappy.parquet</td><td>4730</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-8c13db93-b59a-46a9-91de-7a847de8bcab-c000.snappy.parquet</td><td>part-00000-8c13db93-b59a-46a9-91de-7a847de8bcab-c000.snappy.parquet</td><td>4715</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-8f6e8d44-ffdb-46d0-a3fb-1f1cdfd6c91b-c000.snappy.parquet</td><td>part-00000-8f6e8d44-ffdb-46d0-a3fb-1f1cdfd6c91b-c000.snappy.parquet</td><td>4748</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-90f0c9d1-dbf7-4806-bd37-55910c6ed0b5-c000.snappy.parquet</td><td>part-00000-90f0c9d1-dbf7-4806-bd37-55910c6ed0b5-c000.snappy.parquet</td><td>4758</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-95aeb2fd-0d1e-4e4e-b862-29e6d44b4694-c000.snappy.parquet</td><td>part-00000-95aeb2fd-0d1e-4e4e-b862-29e6d44b4694-c000.snappy.parquet</td><td>4747</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-9744a535-cb3c-4d3a-8881-9ba9c9e1735d-c000.snappy.parquet</td><td>part-00000-9744a535-cb3c-4d3a-8881-9ba9c9e1735d-c000.snappy.parquet</td><td>4714</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-9b18b650-7fbd-4ad5-ae9b-5ab5487f3094-c000.snappy.parquet</td><td>part-00000-9b18b650-7fbd-4ad5-ae9b-5ab5487f3094-c000.snappy.parquet</td><td>4759</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-9cbaa3d5-f195-46f9-8adb-9c9e3af26bc0-c000.snappy.parquet</td><td>part-00000-9cbaa3d5-f195-46f9-8adb-9c9e3af26bc0-c000.snappy.parquet</td><td>4755</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-a31609e3-fe3d-4437-82d9-449844342c45-c000.snappy.parquet</td><td>part-00000-a31609e3-fe3d-4437-82d9-449844342c45-c000.snappy.parquet</td><td>4754</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-a608fbd1-54d3-4186-9d98-995fdd1ce30e-c000.snappy.parquet</td><td>part-00000-a608fbd1-54d3-4186-9d98-995fdd1ce30e-c000.snappy.parquet</td><td>4697</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-a7701625-b389-4b39-b989-6839dab24f5d-c000.snappy.parquet</td><td>part-00000-a7701625-b389-4b39-b989-6839dab24f5d-c000.snappy.parquet</td><td>4706</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-b2104512-db0e-4e25-9043-74004a39de2b-c000.snappy.parquet</td><td>part-00000-b2104512-db0e-4e25-9043-74004a39de2b-c000.snappy.parquet</td><td>4750</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-b6023e49-7a79-4d57-b711-b503eec1b4c7-c000.snappy.parquet</td><td>part-00000-b6023e49-7a79-4d57-b711-b503eec1b4c7-c000.snappy.parquet</td><td>4732</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-b6490d3c-c790-4788-9b00-f56b7f51284b-c000.snappy.parquet</td><td>part-00000-b6490d3c-c790-4788-9b00-f56b7f51284b-c000.snappy.parquet</td><td>4767</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-b7e44b73-dba3-4eb0-927a-75ff2c246335-c000.snappy.parquet</td><td>part-00000-b7e44b73-dba3-4eb0-927a-75ff2c246335-c000.snappy.parquet</td><td>4725</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-bac8f2f5-a7f6-4237-8711-8432c07316bc-c000.snappy.parquet</td><td>part-00000-bac8f2f5-a7f6-4237-8711-8432c07316bc-c000.snappy.parquet</td><td>4743</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-bb40a23a-2f7a-492c-be53-7c0caea8f9b7-c000.snappy.parquet</td><td>part-00000-bb40a23a-2f7a-492c-be53-7c0caea8f9b7-c000.snappy.parquet</td><td>4714</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-c07e3e5a-116b-4651-a790-827c6dadad77-c000.snappy.parquet</td><td>part-00000-c07e3e5a-116b-4651-a790-827c6dadad77-c000.snappy.parquet</td><td>4727</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-c2814f1d-a9db-4d0f-8abd-3ea5aa1225f5-c000.snappy.parquet</td><td>part-00000-c2814f1d-a9db-4d0f-8abd-3ea5aa1225f5-c000.snappy.parquet</td><td>4725</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-c49ccac7-27c6-4af7-8bb6-2759e42c3a72-c000.snappy.parquet</td><td>part-00000-c49ccac7-27c6-4af7-8bb6-2759e42c3a72-c000.snappy.parquet</td><td>4743</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-c52a0a42-7cc0-481d-9339-785353cb2045-c000.snappy.parquet</td><td>part-00000-c52a0a42-7cc0-481d-9339-785353cb2045-c000.snappy.parquet</td><td>4751</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-c69e44a2-6065-4889-a768-8a84fab0cb2b-c000.snappy.parquet</td><td>part-00000-c69e44a2-6065-4889-a768-8a84fab0cb2b-c000.snappy.parquet</td><td>4729</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-c7b71095-511d-4f89-bfcc-41774160e8b9-c000.snappy.parquet</td><td>part-00000-c7b71095-511d-4f89-bfcc-41774160e8b9-c000.snappy.parquet</td><td>4744</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-cb02cb1c-9b8b-47e1-9650-d238fc2b1e6b-c000.snappy.parquet</td><td>part-00000-cb02cb1c-9b8b-47e1-9650-d238fc2b1e6b-c000.snappy.parquet</td><td>4710</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-cefe7117-080f-42b3-864b-ba9e7b0cda0b-c000.snappy.parquet</td><td>part-00000-cefe7117-080f-42b3-864b-ba9e7b0cda0b-c000.snappy.parquet</td><td>4742</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-d2ee977e-b198-4daa-8ab2-7f0f8e178099-c000.snappy.parquet</td><td>part-00000-d2ee977e-b198-4daa-8ab2-7f0f8e178099-c000.snappy.parquet</td><td>4746</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-d3616da3-7dab-433a-a9ae-0162c60dae26-c000.snappy.parquet</td><td>part-00000-d3616da3-7dab-433a-a9ae-0162c60dae26-c000.snappy.parquet</td><td>4753</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-d98f126f-0847-499c-a769-b3ba06c60bc7-c000.snappy.parquet</td><td>part-00000-d98f126f-0847-499c-a769-b3ba06c60bc7-c000.snappy.parquet</td><td>4761</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-da273da6-20ed-4ebf-ba53-c99e4c16d8b8-c000.snappy.parquet</td><td>part-00000-da273da6-20ed-4ebf-ba53-c99e4c16d8b8-c000.snappy.parquet</td><td>4746</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-dc927f89-9c4a-44e8-b3b4-7012cba6f71a-c000.snappy.parquet</td><td>part-00000-dc927f89-9c4a-44e8-b3b4-7012cba6f71a-c000.snappy.parquet</td><td>4766</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-dcdca65e-9279-43db-8319-7ad166150708-c000.snappy.parquet</td><td>part-00000-dcdca65e-9279-43db-8319-7ad166150708-c000.snappy.parquet</td><td>4713</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-dfb46cc9-ca1f-4ad5-ae57-ae731b966623-c000.snappy.parquet</td><td>part-00000-dfb46cc9-ca1f-4ad5-ae57-ae731b966623-c000.snappy.parquet</td><td>4710</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-e1e112b5-f6e2-4eed-b977-0effbe1edf5c-c000.snappy.parquet</td><td>part-00000-e1e112b5-f6e2-4eed-b977-0effbe1edf5c-c000.snappy.parquet</td><td>4749</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-e2eaba8a-b59a-4c43-9b4a-bdc028f6dc98-c000.snappy.parquet</td><td>part-00000-e2eaba8a-b59a-4c43-9b4a-bdc028f6dc98-c000.snappy.parquet</td><td>4699</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-e38c78e0-f0dc-41d4-af83-f13a4ffd71d8-c000.snappy.parquet</td><td>part-00000-e38c78e0-f0dc-41d4-af83-f13a4ffd71d8-c000.snappy.parquet</td><td>4722</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-e447ac2d-8a58-4ce2-9b84-6505afc49190-c000.snappy.parquet</td><td>part-00000-e447ac2d-8a58-4ce2-9b84-6505afc49190-c000.snappy.parquet</td><td>4763</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-e95e222d-383e-479f-b109-cbc38b5bde9b-c000.snappy.parquet</td><td>part-00000-e95e222d-383e-479f-b109-cbc38b5bde9b-c000.snappy.parquet</td><td>4767</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-eaaaabcf-3dfa-40d4-8bec-213cdee10129-c000.snappy.parquet</td><td>part-00000-eaaaabcf-3dfa-40d4-8bec-213cdee10129-c000.snappy.parquet</td><td>4715</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-ed7027e0-dce4-4864-bf13-4904bb3f5a8a-c000.snappy.parquet</td><td>part-00000-ed7027e0-dce4-4864-bf13-4904bb3f5a8a-c000.snappy.parquet</td><td>4762</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-f0701f5a-55d8-4d3b-9306-dfa3d66deee5-c000.snappy.parquet</td><td>part-00000-f0701f5a-55d8-4d3b-9306-dfa3d66deee5-c000.snappy.parquet</td><td>4714</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-f2ee04e6-f63a-4037-90fc-d47eb6bd8d66-c000.snappy.parquet</td><td>part-00000-f2ee04e6-f63a-4037-90fc-d47eb6bd8d66-c000.snappy.parquet</td><td>4705</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-fc8e4630-5053-4974-95fc-26cda96800d3-c000.snappy.parquet</td><td>part-00000-fc8e4630-5053-4974-95fc-26cda96800d3-c000.snappy.parquet</td><td>4719</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-fdc063a6-3a40-415f-a80f-107cecda4286-c000.snappy.parquet</td><td>part-00000-fdc063a6-3a40-415f-a80f-107cecda4286-c000.snappy.parquet</td><td>4718</td></tr><tr><td>dbfs:/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/part-00000-fe2672af-a4ce-4a5b-afa4-d9e659ab2106-c000.snappy.parquet</td><td>part-00000-fe2672af-a4ce-4a5b-afa4-d9e659ab2106-c000.snappy.parquet</td><td>4799</td></tr></tbody></table></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["#### Create a temporary table from the input stream so that you can quickly issue SQL queries against it."],"metadata":{}},{"cell_type":"code","source":["inputDF.createOrReplaceTempView(\"parquet_sensors\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["### Run queries againt the temp table created off the input stream"],"metadata":{}},{"cell_type":"code","source":["%sql select * from parquet_sensors where deviceType = 'SensorTypeD' or deviceType = 'SensorTypeA'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th><th>deviceId</th><th>deviceType</th><th>signalStrength</th><th>INPUT_FILE_NAME</th><th>PROCESSED_TIME</th></tr></thead><tbody><tr><td>2016-08-03T01:32:20.454+0000</td><td>44</td><td>SensorTypeD</td><td>0.6627327869584749</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:11.230+0000</td><td>36</td><td>SensorTypeD</td><td>0.8869519587040092</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:29.490+0000</td><td>6</td><td>SensorTypeA</td><td>0.621775542316748</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:26.601+0000</td><td>31</td><td>SensorTypeA</td><td>0.7217142790760191</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:14.999+0000</td><td>2</td><td>SensorTypeD</td><td>0.9463019453554669</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:19.582+0000</td><td>31</td><td>SensorTypeA</td><td>0.4177194090879053</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:14.446+0000</td><td>61</td><td>SensorTypeA</td><td>0.8093833461353955</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:12.688+0000</td><td>93</td><td>SensorTypeA</td><td>0.6009808303441028</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:13.518+0000</td><td>45</td><td>SensorTypeD</td><td>0.705299532046275</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:15.719+0000</td><td>68</td><td>SensorTypeD</td><td>0.39902827917907346</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:13.189+0000</td><td>1</td><td>SensorTypeD</td><td>0.8394458690136491</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:22.277+0000</td><td>38</td><td>SensorTypeA</td><td>0.5329695475033495</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:28.030+0000</td><td>72</td><td>SensorTypeD</td><td>0.9283179220154705</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:30.085+0000</td><td>4</td><td>SensorTypeD</td><td>0.963006716111817</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:11.460+0000</td><td>50</td><td>SensorTypeD</td><td>0.6703599347895163</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:16.509+0000</td><td>44</td><td>SensorTypeA</td><td>0.22434359917372337</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:27.512+0000</td><td>86</td><td>SensorTypeA</td><td>0.8566790864779773</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:17.750+0000</td><td>29</td><td>SensorTypeA</td><td>0.03596722780013162</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:20.287+0000</td><td>74</td><td>SensorTypeD</td><td>0.04349601929527647</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:17.166+0000</td><td>9</td><td>SensorTypeA</td><td>0.9532572367956021</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:21.761+0000</td><td>31</td><td>SensorTypeA</td><td>0.7469761422871205</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:18.463+0000</td><td>85</td><td>SensorTypeA</td><td>0.9784681554516804</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:27.905+0000</td><td>69</td><td>SensorTypeD</td><td>0.891029169033982</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:24.007+0000</td><td>71</td><td>SensorTypeA</td><td>0.5422545547994615</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:17.039+0000</td><td>47</td><td>SensorTypeD</td><td>0.162245219322937</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:15.142+0000</td><td>94</td><td>SensorTypeD</td><td>0.4040257715867749</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:18.388+0000</td><td>36</td><td>SensorTypeA</td><td>0.6523832270384281</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:12.509+0000</td><td>55</td><td>SensorTypeD</td><td>0.12299331091076948</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:25.408+0000</td><td>92</td><td>SensorTypeD</td><td>0.9118539675090813</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:28.450+0000</td><td>43</td><td>SensorTypeD</td><td>0.5439255295271633</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:21.653+0000</td><td>83</td><td>SensorTypeD</td><td>0.06821574311029666</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:14.740+0000</td><td>76</td><td>SensorTypeA</td><td>0.010231024368771102</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:25.754+0000</td><td>73</td><td>SensorTypeA</td><td>0.43604706651423075</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:16.085+0000</td><td>95</td><td>SensorTypeA</td><td>0.2695919341224231</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:28.020+0000</td><td>58</td><td>SensorTypeD</td><td>0.9046288749752396</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:17.725+0000</td><td>46</td><td>SensorTypeA</td><td>0.09691718492352108</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:13.361+0000</td><td>39</td><td>SensorTypeD</td><td>0.8162160449765161</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:27.362+0000</td><td>46</td><td>SensorTypeD</td><td>0.8365669642000434</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:26.872+0000</td><td>94</td><td>SensorTypeD</td><td>0.8175131388325416</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:26.451+0000</td><td>37</td><td>SensorTypeA</td><td>0.9875242404993146</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:27.597+0000</td><td>76</td><td>SensorTypeA</td><td>0.21949461585534946</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:17.380+0000</td><td>74</td><td>SensorTypeD</td><td>0.26605415173847935</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:20.131+0000</td><td>37</td><td>SensorTypeA</td><td>0.03716060426292367</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:30.240+0000</td><td>4</td><td>SensorTypeA</td><td>0.2304273672345647</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:29.345+0000</td><td>63</td><td>SensorTypeA</td><td>0.05922425836866729</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:28.574+0000</td><td>10</td><td>SensorTypeA</td><td>0.36783573100564315</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:19.852+0000</td><td>62</td><td>SensorTypeD</td><td>0.14911421884394238</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:18.084+0000</td><td>85</td><td>SensorTypeD</td><td>0.7363900468491809</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:28.157+0000</td><td>46</td><td>SensorTypeA</td><td>0.03325111267380665</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:14.622+0000</td><td>47</td><td>SensorTypeD</td><td>0.919507310823648</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:18.015+0000</td><td>1</td><td>SensorTypeD</td><td>0.1280139156865766</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:17.827+0000</td><td>66</td><td>SensorTypeA</td><td>0.8363862971430444</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:20.441+0000</td><td>50</td><td>SensorTypeD</td><td>0.9531315112969936</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:25.505+0000</td><td>95</td><td>SensorTypeA</td><td>0.5284995580808108</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:13.213+0000</td><td>49</td><td>SensorTypeA</td><td>0.9806847404571597</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr><tr><td>2016-08-03T01:32:14.832+0000</td><td>38</td><td>SensorTypeD</td><td>0.15881318095677788</td><td>dbfs:/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/streaming-sensor_file-1.json</td><td>2020-02-11T02:37:16.021+0000</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["(Click Run All Above Here)\n\n(Then Individually run below)"],"metadata":{}},{"cell_type":"markdown","source":["### Run queries and additional processing against Parquet file stored from the input stream"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\") #keep the size of shuffles small for better query performance\ndevices = (spark.readStream\n           .schema(parquetSchema)\n           .format(\"parquet\")\n           .option(\"maxFilesPerTrigger\", 1) #slow it down to demo\n           .load(output_path)\n           .withWatermark(\"PROCESSED_TIME\", \"1 minute\") #window for out of order data\n          )\n\n # generate temp table for more complex aggregation queries\ndevices.createOrReplaceTempView(\"sensors\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["#### What files are being processed?"],"metadata":{}},{"cell_type":"code","source":["display(\n  devices.\n  select(\"INPUT_FILE_NAME\", \"PROCESSED_TIME\")\n  .groupBy(\"INPUT_FILE_NAME\", \"PROCESSED_TIME\")\n  .count()\n  .orderBy(\"PROCESSED_TIME\", ascending=False)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["#### How much data is coming through?"],"metadata":{}},{"cell_type":"code","source":["%sql select count(*) from sensors"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["#### What does the min, max, and average strength look like for each sensor type?\n\nUse Spark SQL functions min(), max(), and avg()\n\nNote: I can use SQL within my Python notebook via `%sql` magic command"],"metadata":{}},{"cell_type":"code","source":["%sql \n\nselect count(*), deviceType, min(signalStrength), max(signalStrength), avg(signalStrength) \n  from sensors \n    group by deviceType \n    order by deviceType asc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["#### Let's create a stream to aggregate signal counts by device and timestamp over 5 second windows.\n\nNote: this is tumbling window, not a sliding window; its size is 5 seconds\n\nFor example, discounting day, hours, and min, a tumbling window of size 5 looks as follows:\n\n*[(00:00 - 00:05), (00:05: 00:10), (00:10: 00:15)]*\n\nAn event could fall into any of these tumbling windows."],"metadata":{}},{"cell_type":"code","source":["(devices\n .groupBy(\n   window(\"timestamp\", \"5 seconds\"),\n   \"deviceId\"\n )\n .count()\n .createOrReplaceTempView(\"sensor_counts\")) #create a temporary view atop DataFrame"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["#### Which devices are experiencing signal loss over those 5 second windows?"],"metadata":{}},{"cell_type":"code","source":["%sql select * from sensor_counts where count < 5 order by window.start desc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["### Send Alerts for potentially down sensors that have not sent signals\n\nLet's create a DataFrame from our temporary table `sensor_counts`"],"metadata":{}},{"cell_type":"code","source":["lost_sensor_signals = (spark.table(\"sensor_counts\")\n         .filter(col(\"count\") < 5)\n         .select(\"window.start\", \"window.end\", \"deviceId\", \"count\")\n         )\n\n#display our DataFrame\ndisplay(lost_sensor_signals)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["##### Using `foreach` mechanism to write to workers' logs.\n\nThis could be used for monitoring purposes. Either another job scanning the logs for ALERTS\nand publishing them onto a topic on Kafka or posting them on Ganglia. A good excerice to try\nif you have a Kafka cluster aviable or Ganglia service available via a REST API."],"metadata":{}},{"cell_type":"code","source":["def processRow(row):\n  # for now write them to log files, but this logic can easily be extended to publishing alerts\n  # to a topic on Kafka or monitoring/paging service such as Ganglia or PagerDuty.\n  print(\"ALERT from Sensors: Between {} and {}, device {} reported only {} times\".format(row.start, row.end, row.deviceId, row[3]))\n  \n(lost_sensor_signals\n .writeStream\n .outputMode(\"complete\") #could be our Kafka topic \"alerts\" for monitoring\n .foreach(processRow)\n .start()\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2024864360423174&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>  <span class=\"ansi-blue-fg\">.</span>writeStream\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>  <span class=\"ansi-blue-fg\">.</span>outputMode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;complete&#34;</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\">#could be our Kafka topic &#34;alerts&#34; for monitoring</span>\n<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\">  </span><span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span>processRow<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span>  <span class=\"ansi-blue-fg\">.</span>start<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> )\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/streaming.py</span> in <span class=\"ansi-cyan-fg\">start</span><span class=\"ansi-blue-fg\">(self, path, format, outputMode, partitionBy, queryName, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1120</span>             self<span class=\"ansi-blue-fg\">.</span>queryName<span class=\"ansi-blue-fg\">(</span>queryName<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1121</span>         <span class=\"ansi-green-fg\">if</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1122</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_sq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>start<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1123</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1124</span>             <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_sq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>start<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o652.start.\n: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:123)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:83)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:883)\n\tat com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:3561)\n\tat com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:2909)\n\tat org.json4s.jackson.Serialization$.write(Serialization.scala:27)\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:80)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:128)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:126)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.&lt;init&gt;(StreamExecution.scala:126)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.&lt;init&gt;(MicroBatchExecution.scala:52)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:295)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:336)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:290)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:261)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:255)\n\tat com.databricks.api.rpc.ScalaProtoRpcSerializer.deserializeException(ScalaProtoRpcSerializer.scala:16)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:252)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:251)\n\tat com.databricks.api.rpc.ScalaProtoRpcSerializer.deserializeException(ScalaProtoRpcSerializer.scala:16)\n\tat com.databricks.rpc.JettyClient.$anonfun$getResponseFromExchange$4(JettyClient.scala:327)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:34)\n\tat com.databricks.rpc.JettyClient.getResponseFromExchange(JettyClient.scala:325)\n\tat com.databricks.rpc.JettyClient.$anonfun$processResponse$1(JettyClient.scala:356)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.rpc.AbstractJettyClient.withAttributionContext(BaseJettyClient.scala:85)\n\tat com.databricks.rpc.JettyClient.com$databricks$rpc$JettyClient$$processResponse(JettyClient.scala:353)\n\tat com.databricks.rpc.JettyClient.$anonfun$send$1(JettyClient.scala:102)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyClient.send(JettyClient.scala:102)\n\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:559)\n\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:36)\n\tat com.databricks.rpc.ReliableJettyClient.$anonfun$sendNonIdempotent$2(ReliableJettyClient.scala:101)\n\tat com.databricks.rpc.ReliableJettyClient.retryOnTransientError(ReliableJettyClient.scala:195)\n\tat com.databricks.rpc.ReliableJettyClient.sendNonIdempotent(ReliableJettyClient.scala:101)\n\tat com.databricks.backend.daemon.data.server.DbfsLimitEnforcer.allocate(DbfsLimitEnforcer.scala:26)\n\tat com.databricks.s3a.enforcer.S3AEnforcer$.allocateWithOverwriteCheck(S3AEnforcer.scala:42)\n\tat com.databricks.s3a.aws.transfer.EnforcingDatabricksTransferManager.upload(EnforcingDatabricksTransferManager.scala:42)\n\tat com.databricks.s3a.S3AOutputStream$1.apply(S3AOutputStream.java:148)\n\tat com.databricks.s3a.S3AOutputStream$1.apply(S3AOutputStream.java:143)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.s3a.aws.DatabricksS3Client.retryRequest(DatabricksS3Client.scala:151)\n\tat com.databricks.s3a.aws.DatabricksS3Client.withExponentialBackoff(DatabricksS3Client.scala:125)\n\tat com.databricks.s3a.aws.DatabricksS3Client.withExponentialBackoff$(DatabricksS3Client.scala:120)\n\tat com.databricks.s3a.aws.EnforcingDatabricksS3Client.withExponentialBackoff(EnforcingDatabricksS3Client.scala:28)\n\tat com.databricks.s3a.S3AOutputStream.close(S3AOutputStream.java:143)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat com.databricks.backend.daemon.data.server.session.IOStreamManager$OutStream.close(IOStreamManager.scala:154)\n\tat com.databricks.backend.daemon.data.server.handler.IOStreamHandler.receive(IOStreamHandler.scala:37)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:103)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:102)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:102)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:302)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:280)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:52)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:79)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:79)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:48)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:428)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:15)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:15)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:409)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:336)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:15)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:47)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:563)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:563)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:489)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:301)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:166)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:166)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:290)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:202)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["_Here is an example of the results being written out._\n\n<img src=\"https://databricks.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-18-at-5.53.39-PM.png\" alt=\"\" width=\"50%\"/>"],"metadata":{}},{"cell_type":"markdown","source":["##### CHALLENGE-1: \n\nTry using`foreachBatch` to write each micro-batch using standard DataFrame API.\n\n__See [docs](https://docs.databricks.com/spark/latest/structured-streaming/foreach.html).__"],"metadata":{}},{"cell_type":"code","source":["def foreach_batch_function(df, epoch_id):\n    # Transform and write batchDF\n    for row in df.rdd.collect():\n      print(\"ALERT from Sensors: Between {} and {}, device {} reported only {} times\".format(row.start, row.end, row.deviceId, row[3]))\n  \n(lost_sensor_signals\n .writeStream\n .outputMode(\"complete\") #could be our Kafka topic \"alerts\" for monitoring\n .foreachBatch(foreach_batch_function)\n .start()\n)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["##### CHALLENGE-2: \n\nTry adding additional monitoring conditions on devices. For example, add another streaming query that\nfilters monitoring conditions (signal strength < 0.2) from any of the above streams or tables: sensor_counts, devices, or inputStream, or parquet files.\n\n__See [docs](https://docs.databricks.com/spark/latest/structured-streaming/foreach.html).__"],"metadata":{}},{"cell_type":"code","source":["low_signal_strength = (spark.table(\"parquet_sensors\")\n         .filter(col('signalStrength') < 0.2)\n         .select(\"timestamp\", \"deviceId\", \"deviceType\", \"signalStrength\")\n         )\n\n#display our DataFrame\ndisplay(low_signal_strength)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["def processRow1(row):\n  # for now write them to log files, but this logic can easily be extended to publishing alerts\n  # to a topic on Kafka or monitoring/paging service such as Ganglia or PagerDuty.\n  print(\"ALERT from Sensors: at {} device {} reported a low signal strength of {}\".format(row.timestamp, row.deviceId, row.signalStrength))\n  \n(low_signal_strength\n .writeStream\n #.outputMode(\"complete\") #could be our Kafka topic \"alerts\" for monitoring\n .foreach(processRow1)\n .start()\n)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["## Cleanup Data"],"metadata":{}},{"cell_type":"code","source":["%run \"./setup/cleanup_data\""],"metadata":{},"outputs":[],"execution_count":50}],"metadata":{"name":"1-continuous_application_sensor","notebookId":2024864360423130},"nbformat":4,"nbformat_minor":0}
